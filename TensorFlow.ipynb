{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tensorflow in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\mahendravishnuprabha\\.conda\\envs\\my_python\\lib\\site-packages (0.0.post10)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Patrick', shape=(), dtype=string)\n",
      "tf.Tensor([b'Patrick' b'Max' b'Mary'], shape=(3,), dtype=string)\n",
      "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n"
     ]
    }
   ],
   "source": [
    "# Beginner Course\n",
    "# Tensor is like a numpy array, we can convert a tensor to a numpy array and vice versa\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# rank -1 tensor\n",
    "# x = tf.constant(4, shape = (1, 1), dtype = tf.float32)\n",
    "\n",
    "# rank -2 tensor\n",
    "# x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# x = tf.ones((3, 3))\n",
    "\n",
    "# x = tf.eye(3)\n",
    "\n",
    "# x = tf.random.normal((3, 3), mean = 0, stddev = 1)\n",
    "\n",
    "# x = tf.random.uniform((3, 3), minval = 0, maxval = 1)\n",
    "\n",
    "# x = tf.range(10)\n",
    "# print(x)\n",
    "\n",
    "# cast\n",
    "\n",
    "# x = tf.cast(x, dtype = tf.float32)\n",
    "# print(x)\n",
    "\n",
    "\n",
    "# elementwise addition\n",
    "\n",
    "x = tf.constant([1, 2, 3])\n",
    "y = tf.constant([4, 5, 6])\n",
    "\n",
    "# z = tf.add(x, y)\n",
    "# or\n",
    "# z = x + y\n",
    "\n",
    "# elementwise subtraction\n",
    "\n",
    "z = tf.subtract(y, x)\n",
    "# or\n",
    "# z = x - y\n",
    "\n",
    "# elementwise divide\n",
    "\n",
    "z = tf.divide(y, x)\n",
    "# or\n",
    "# z = x / y\n",
    "\n",
    "# elementwise multiplication\n",
    "\n",
    "z = tf.multiply(x, y)\n",
    "# or\n",
    "# z = x * y\n",
    "\n",
    "# elementwise dotproduct\n",
    "\n",
    "z = tf.tensordot(x, y, axes = 1)\n",
    "\n",
    "# elementwise exponential\n",
    "\n",
    "z = x ** 3\n",
    "\n",
    "# elementwise matrix multiply - the no. of columns in first varaible should be the same as the no of rows in 2nd variable\n",
    "\n",
    "# a = tf.random.normal((row, column))\n",
    "\n",
    "# x = tf.random.normal((2,2)) \n",
    "# y = tf.random.normal((2,2))\n",
    "\n",
    "# z = tf.matmul(x, y)\n",
    "# or\n",
    "# z = x @ y\n",
    "\n",
    "# print(z)\n",
    "\n",
    "# slicing , indexing\n",
    "\n",
    "x = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "\n",
    "# all the rows and only column 1\n",
    "# print(x[:, 1])\n",
    "\n",
    "# all the columns and only row 1\n",
    "# print(x[1, :])\n",
    "\n",
    "# from row 1 we want index 1, 2\n",
    "# print(x[1, 1:3])\n",
    "\n",
    "# from row 1 we want to access the number in column or index number 1\n",
    "# print(x[1, 1])\n",
    "\n",
    "# reshaping\n",
    "\n",
    "x = tf.random.normal((2,3)) \n",
    "# print(x)\n",
    "\n",
    "# x = tf.reshape(x, (3, 2))\n",
    "# print(x)\n",
    "\n",
    "# convert to numpy\n",
    "\n",
    "# x = tf.random.normal((2,3)) \n",
    "# print(x)\n",
    "\n",
    "# x = x.numpy()\n",
    "# print(x)\n",
    "# print(type(x))\n",
    "\n",
    "# x = tf.convert_to_tensor(x)\n",
    "# print(type(x))\n",
    "\n",
    "# string tensor\n",
    "x = tf.constant(\"Patrick\")\n",
    "print(x)\n",
    "\n",
    "x = tf.constant([\"Patrick\", \"Max\", \"Mary\"])\n",
    "print(x)\n",
    "\n",
    "# Variable\n",
    "# A tf.Variable represents a tensor whose value can be\n",
    "# changed by running ops on it\n",
    "# Used to represent shared, persistent state your program manipulates\n",
    "# Higher level libraries like tf.keras use tf.Variable to store model parameters.\n",
    "b = tf.Variable([[1.0, 2.0, 3.0]])\n",
    "print(b)\n",
    "print(type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_9 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "938/938 - 3s - loss: 0.2986 - accuracy: 0.9159 - 3s/epoch - 3ms/step\n",
      "Epoch 2/5\n",
      "938/938 - 2s - loss: 0.1364 - accuracy: 0.9605 - 2s/epoch - 2ms/step\n",
      "Epoch 3/5\n",
      "938/938 - 2s - loss: 0.0964 - accuracy: 0.9716 - 2s/epoch - 3ms/step\n",
      "Epoch 4/5\n",
      "938/938 - 3s - loss: 0.0744 - accuracy: 0.9783 - 3s/epoch - 3ms/step\n",
      "Epoch 5/5\n",
      "938/938 - 3s - loss: 0.0588 - accuracy: 0.9827 - 3s/epoch - 3ms/step\n",
      "157/157 - 0s - loss: 0.0840 - accuracy: 0.9739 - 460ms/epoch - 3ms/step\n",
      "tf.Tensor([0.    0.    0.    0.004 0.    0.    0.    0.996 0.    0.   ], shape=(10,), dtype=float32)\n",
      "7\n",
      "tf.Tensor([0.    0.    0.    0.004 0.    0.    0.    0.996 0.    0.   ], shape=(10,), dtype=float32)\n",
      "7\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "tf.Tensor([0.    0.    0.    0.004 0.    0.    0.    0.996 0.    0.   ], shape=(10,), dtype=float32)\n",
      "7\n",
      "(5, 10)\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# First Neural Net\n",
    "# Train, evaluate, and predict with the model\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "# normalize: 0,255 -> 0,1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# plotting mnist\n",
    "\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2, 3, i + 1)\n",
    "#     plt.imshow(x_train[i], cmap = \"gray\")\n",
    "# plt.show()\n",
    "\n",
    "# model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# another way to build the Sequential model:\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Flatten(input_shape=(28,28))\n",
    "#model.add(keras.layers.Dense(128, activation='relu'))\n",
    "#model.add(keras.layers.Dense(10))\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "# training\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2)\n",
    "\n",
    "# evaulate\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# predictions\n",
    "\n",
    "# 1. option: build new model with Softmax layer\n",
    "probability_model = keras.models.Sequential([\n",
    "    model,\n",
    "    keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "predictions = probability_model(x_test)\n",
    "pred0 = predictions[0]\n",
    "print(pred0)\n",
    "\n",
    "# use np.argmax to get label with highest probability\n",
    "label0 = np.argmax(pred0)\n",
    "print(label0)\n",
    "\n",
    "# 2. option: original model + nn.softmax, call model(x)\n",
    "predictions = model(x_test)\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "pred0 = predictions[0]\n",
    "print(pred0)\n",
    "label0 = np.argmax(pred0)\n",
    "print(label0)\n",
    "\n",
    "# 3. option: original model + nn.softmax, call model.predict(x)\n",
    "predictions = model.predict(x_test, batch_size=batch_size)\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "pred0 = predictions[0]\n",
    "print(pred0)\n",
    "label0 = np.argmax(pred0)\n",
    "print(label0)\n",
    "\n",
    "# call argmax for multiple labels\n",
    "pred05s = predictions[0:5]\n",
    "print(pred05s.shape)\n",
    "label05s = np.argmax(pred05s, axis=1)\n",
    "print(label05s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 10) (314, 10) (78, 10)\n",
      "                     mean         std\n",
      "MPG             23.310510    7.728652\n",
      "Cylinders        5.477707    1.699788\n",
      "Displacement   195.318471  104.331589\n",
      "Horsepower     104.869427   38.096214\n",
      "Weight        2990.251592  843.898596\n",
      "Acceleration    15.559236    2.789230\n",
      "Model Year      75.898089    3.675642\n",
      "USA              0.624204    0.485101\n",
      "Europe           0.178344    0.383413\n",
      "Japan            0.197452    0.398712\n",
      "[[   5.478  195.318  104.869 2990.252   15.559   75.898    0.624    0.178\n",
      "     0.197]]\n",
      "First example: [[   4.    90.    75.  2125.    14.5   74.     1.     0.     0. ]]\n",
      "Normalized: [[-0.871 -1.011 -0.785 -1.027 -0.38  -0.517  0.776 -0.466 -0.496]]\n",
      "(314,) (314, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 122, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 295, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\", line 191, in build\n        self.axis, input_shape, d\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20280\\329904809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m# adapt to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[0msingle_feature_normalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;31m# Sequential model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    284\u001b[0m               \u001b[0margument\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupported\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0marray\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \"\"\"\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                         \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36mtf__adapt_step\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunctionScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adapt_step'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fscope'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_requested\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_maybe_build\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf__adapt_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36m_adapt_maybe_build\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;31m# Set the number of dimensions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape_nones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    189\u001b[0m                     \u001b[1;34m\"Got axis: {}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                     \"input shape: {}, with unknown axis at index: {}\".format(\n\u001b[1;32m--> 191\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m                     )\n\u001b[0;32m    193\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 122, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 295, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"c:\\Users\\MahendraVishnuPrabha\\.conda\\envs\\my_python\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\", line 191, in build\n        self.axis, input_shape, d\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression - Full Project Walkthrough\n",
    "\n",
    "# Basic regression to predict fuel efficiency\n",
    "# Code is based on this tutorial: https://www.tensorflow.org/tutorials/keras/regression\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # pip install pandas\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "dataset = pd.read_csv(url, names=column_names, na_values='?',\n",
    "                      comment='\\t', sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset.tail()\n",
    "\n",
    "# clean data\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# convert categorical 'Origin' data into one-hot data\n",
    "origin = dataset.pop('Origin')\n",
    "dataset['USA'] = (origin == 1)*1\n",
    "dataset['Europe'] = (origin == 2)*1\n",
    "dataset['Japan'] = (origin == 3)*1\n",
    "\n",
    "dataset.tail()\n",
    "\n",
    "# Split the data into train and test\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(dataset.shape, train_dataset.shape, test_dataset.shape)\n",
    "train_dataset.describe().transpose()\n",
    "\n",
    "# split features from labels\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')\n",
    "\n",
    "def plot(feature, x=None, y=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(train_features[feature], train_labels, label='Data')\n",
    "    if x is not None and y is not None:\n",
    "        plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()\n",
    "\n",
    "# plot('Horsepower')\n",
    "\n",
    "# plot('Weight')\n",
    "\n",
    "# Normalize\n",
    "print(train_dataset.describe().transpose()[['mean', 'std']])\n",
    "\n",
    "# Normalization\n",
    "normalizer = preprocessing.Normalization()\n",
    "\n",
    "# adapt to the data\n",
    "normalizer.adapt(np.array(train_features))\n",
    "print(normalizer.mean.numpy())\n",
    "\n",
    "# When the layer is called it returns the input data, with each feature independently normalized:\n",
    "# (input-mean)/stddev\n",
    "first = np.array(train_features[:1])\n",
    "print('First example:', first)\n",
    "print('Normalized:', normalizer(first).numpy())\n",
    "\n",
    "# Regression\n",
    " # 1. Normalize the input horsepower\n",
    " # 2. Apply a linear transformation (y = m*x+b) to produce 1 output using layers.Dense\n",
    "\n",
    "feature = 'Horsepower'\n",
    "single_feature = np.array(train_features[feature])\n",
    "print(single_feature.shape, train_features.shape)\n",
    "\n",
    "# Normalization\n",
    "single_feature_normalizer = preprocessing.Normalization()\n",
    "\n",
    "# adapt to the data\n",
    "single_feature_normalizer.adapt(single_feature)\n",
    "\n",
    "# Sequential model\n",
    "single_feature_model = keras.models.Sequential([\n",
    "    single_feature_normalizer,\n",
    "    layers.Dense(units=1) # Linear Model\n",
    "])\n",
    "\n",
    "single_feature_model.summary()\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.MeanAbsoluteError() # MeanSquaredError\n",
    "optim = keras.optimizers.Adam(lr=0.1)\n",
    "\n",
    "single_feature_model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "history = single_feature_model.fit(\n",
    "    train_features[feature], train_labels,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2)\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 25])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plot_loss(history)\n",
    "\n",
    "\n",
    "single_feature_model.evaluate(\n",
    "    test_features[feature],\n",
    "    test_labels, verbose=1)\n",
    "\n",
    "\n",
    "# predict and plot\n",
    "range_min = np.min(test_features[feature]) - 10\n",
    "range_max = np.max(test_features[feature]) + 10\n",
    "x = tf.linspace(range_min, range_max, 200)\n",
    "y = single_feature_model.predict(x)\n",
    "    \n",
    "# plot(feature, x,y)\n",
    "\n",
    "# DNN\n",
    "dnn_model = keras.Sequential([\n",
    "    single_feature_normalizer,\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "dnn_model.compile(loss=loss, \n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "dnn_model.summary()\n",
    "\n",
    "\n",
    "dnn_model.fit(\n",
    "    train_features[feature], train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=1, epochs=100)\n",
    "\n",
    "\n",
    "dnn_model.evaluate(test_features[feature], test_labels, verbose=1)\n",
    "\n",
    "\n",
    "# predict and plot\n",
    "x = tf.linspace(range_min, range_max, 200)\n",
    "y = dnn_model.predict(x)\n",
    "\n",
    "# plot(feature, x,y)\n",
    "\n",
    "# multiple inputs\n",
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "linear_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=loss)\n",
    "\n",
    "\n",
    "linear_model.fit(\n",
    "    train_features, train_labels, \n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2)\n",
    "\n",
    "\n",
    "linear_model.evaluate(\n",
    "    test_features, test_labels, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 15, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                73792     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,586\n",
      "Trainable params: 84,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "782/782 - 33s - loss: 1.5563 - accuracy: 0.4465 - 33s/epoch - 43ms/step\n",
      "Epoch 2/20\n",
      "782/782 - 32s - loss: 1.2462 - accuracy: 0.5633 - 32s/epoch - 41ms/step\n",
      "Epoch 3/20\n",
      "782/782 - 29s - loss: 1.1291 - accuracy: 0.6069 - 29s/epoch - 37ms/step\n",
      "Epoch 4/20\n",
      "782/782 - 28s - loss: 1.0511 - accuracy: 0.6337 - 28s/epoch - 36ms/step\n",
      "Epoch 5/20\n",
      "782/782 - 29s - loss: 0.9972 - accuracy: 0.6530 - 29s/epoch - 37ms/step\n",
      "Epoch 6/20\n",
      "782/782 - 32s - loss: 0.9446 - accuracy: 0.6725 - 32s/epoch - 41ms/step\n",
      "Epoch 7/20\n",
      "782/782 - 35s - loss: 0.9031 - accuracy: 0.6859 - 35s/epoch - 45ms/step\n",
      "Epoch 8/20\n",
      "782/782 - 33s - loss: 0.8665 - accuracy: 0.7012 - 33s/epoch - 42ms/step\n",
      "Epoch 9/20\n",
      "782/782 - 29s - loss: 0.8364 - accuracy: 0.7090 - 29s/epoch - 38ms/step\n",
      "Epoch 10/20\n",
      "782/782 - 29s - loss: 0.8063 - accuracy: 0.7220 - 29s/epoch - 37ms/step\n",
      "Epoch 11/20\n",
      "782/782 - 29s - loss: 0.7821 - accuracy: 0.7299 - 29s/epoch - 37ms/step\n",
      "Epoch 12/20\n",
      "782/782 - 28s - loss: 0.7547 - accuracy: 0.7395 - 28s/epoch - 36ms/step\n",
      "Epoch 13/20\n",
      "782/782 - 29s - loss: 0.7296 - accuracy: 0.7474 - 29s/epoch - 36ms/step\n",
      "Epoch 14/20\n",
      "782/782 - 31s - loss: 0.7095 - accuracy: 0.7536 - 31s/epoch - 39ms/step\n",
      "Epoch 15/20\n",
      "782/782 - 30s - loss: 0.6854 - accuracy: 0.7618 - 30s/epoch - 39ms/step\n",
      "Epoch 16/20\n",
      "782/782 - 30s - loss: 0.6623 - accuracy: 0.7692 - 30s/epoch - 38ms/step\n",
      "Epoch 17/20\n",
      "782/782 - 31s - loss: 0.6448 - accuracy: 0.7741 - 31s/epoch - 40ms/step\n",
      "Epoch 18/20\n",
      "782/782 - 28s - loss: 0.6197 - accuracy: 0.7836 - 28s/epoch - 36ms/step\n",
      "Epoch 19/20\n",
      "782/782 - 27s - loss: 0.6001 - accuracy: 0.7916 - 27s/epoch - 35ms/step\n",
      "Epoch 20/20\n",
      "782/782 - 29s - loss: 0.5866 - accuracy: 0.7941 - 29s/epoch - 37ms/step\n",
      "157/157 - 2s - loss: 0.9838 - accuracy: 0.6882 - 2s/epoch - 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9837844967842102, 0.6881999969482422]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional Neural Network (CNN) - Tensorflow\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10 = keras.datasets.cifar10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "print(train_images.shape) # 50000, 32, 32, 3\n",
    "\n",
    "# Normalize: 0,255 -> 0,1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# def show():\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     for i in range(16):\n",
    "#         plt.subplot(4,4,i+1)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.grid(False)\n",
    "#         plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "#         # The CIFAR labels happen to be arrays, \n",
    "#         # which is why you need the extra index\n",
    "#         plt.xlabel(class_names[train_labels[i][0]])\n",
    "#     plt.show()\n",
    "\n",
    "# show()\n",
    "\n",
    "# model...\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), strides=(1,1), padding=\"valid\", activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "print(model.summary())\n",
    "#import sys; sys.exit()\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
    "\n",
    "# training\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=epochs,\n",
    "          batch_size=batch_size, verbose=2)\n",
    "\n",
    "# evaulate\n",
    "model.evaluate(test_images,  test_labels, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Load Models\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalize\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "# Feed forward neural network\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "# config\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(lr=0.001) # \"adam\"\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()] # \"accuracy\"\n",
    "\n",
    "# compile\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "# fit/training\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, shuffle=True, verbose=2)\n",
    "\n",
    "print(\"Evaluate:\")\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# 1) Save whole model\n",
    "# two formats: SavedModel or HDF5\n",
    "model.save(\"nn\")  # no file ending = SavedModel\n",
    "model.save(\"nn.h5\")  # .h5 = HDF5\n",
    "\n",
    "new_model = keras.models.load_model(\"nn.h5\")\n",
    "\n",
    "# 2) save only weights\n",
    "model.save_weights(\"nn_weights.h5\")\n",
    "\n",
    "# initilaize model first:\n",
    "# model = keras.Sequential([...])\n",
    "model.load_weights(\"nn_weights.h5\")\n",
    "\n",
    "# 3) save only architecture, to_json\n",
    "json_string = model.to_json()\n",
    "\n",
    "with open(\"nn_model.json\", \"w\") as f:\n",
    "    f.write(json_string)\n",
    "\n",
    "with open(\"nn_model.json\", \"r\") as f:\n",
    "    loaded_json_string = f.read()\n",
    "\n",
    "new_model = keras.models.model_from_json(loaded_json_string)\n",
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API + Multi-output Project\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#  a           a             a     b          a\n",
    "#  |           |              \\    /        /   \\\n",
    "#  b           b                c          b     c\n",
    "#  |          /  \\              |          \\     /\n",
    "#  c         c    d             d             d\n",
    "\n",
    "# model: Sequential: one input, one output\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# create model with functional API\n",
    "# Advantages:\n",
    "#   - Models with multiple inputs and outputs\n",
    "#   - Shared layers\n",
    "#   - Extract and reuse nodes in the graph of layers\n",
    "#   - Model are callable like layers (put model into sequential)\n",
    "# start by creating an Input node\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "\n",
    "flatten = keras.layers.Flatten()\n",
    "dense1 = keras.layers.Dense(128, activation='relu')\n",
    "dense2 = keras.layers.Dense(10)\n",
    "\n",
    "x = flatten(inputs)\n",
    "x = dense1(x)\n",
    "outputs = dense2(x)\n",
    "\n",
    "# or with multiple outputs\n",
    "#dense2_2 = keras.layers.Dense(1)\n",
    "#outputs2 = dense2_2(x)\n",
    "#outputs = [output, outputs2]\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# convert functional to sequential model\n",
    "# only works if the layers graph is linear.\n",
    "new_model = keras.models.Sequential()\n",
    "for layer in model.layers:\n",
    "    new_model.add(layer)\n",
    "    \n",
    "# convert sequential to functional\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "x = new_model.layers[0](inputs)\n",
    "for layer in new_model.layers[1:]:\n",
    "    x = layer(x) \n",
    "outputs = x\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# access inputs, outputs for model\n",
    "# access input + output for layer\n",
    "# access all layers\n",
    "inputs = model.inputs\n",
    "outputs = model.outputs\n",
    "print(inputs)\n",
    "print(outputs)\n",
    "\n",
    "input0 = model.layers[0].input\n",
    "output0 = model.layers[0].output\n",
    "print(input0)\n",
    "print(output0)\n",
    "\n",
    "# Example: Transfer Learning:\n",
    "base_model = keras.applications.VGG16()\n",
    "\n",
    "x = base_model.layers[-2].output\n",
    "new_outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "new_model = keras.Model(inputs=base_model.inputs, outputs=new_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# download the data from kaggle: \n",
    "# https://www.kaggle.com/ihelon/lego-minifigures-tensorflow-tutorial\n",
    "# move the folder into your project folder and create a backup of\n",
    "# the star-wars images at 'lego/star-wars-images/'\n",
    "BASE_DIR = 'C:\\\\Users\\\\MahendraVishnuPrabha\\\\Downloads\\\\TensorFlow\\\\star-wars'\n",
    "\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)\n",
    "\n",
    "names = [\"YODA\", \"LUKE SKYWALKER\", \"R2-D2\", \"MACE WINDU\", \"GENERAL GRIEVOUS\"]\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Reorganize the folder structure:\n",
    "if not os.path.isdir(BASE_DIR + 'train/'):\n",
    "    for name in names:\n",
    "        os.makedirs(BASE_DIR + 'train/' + name)\n",
    "        os.makedirs(BASE_DIR + 'val/' + name)\n",
    "        os.makedirs(BASE_DIR + 'test/' + name)\n",
    "\n",
    "# Moce the image files\n",
    "orig_folders = [\"0001/\", \"0002/\", \"0003/\", \"0004/\", \"0005/\"]\n",
    "for folder_idx, folder in enumerate(orig_folders):\n",
    "    files = os.listdir(BASE_DIR + folder)\n",
    "    number_of_images = len([name for name in files])\n",
    "    n_train = int((number_of_images * 0.6) + 0.5)\n",
    "    n_valid = int((number_of_images*0.25) + 0.5)\n",
    "    n_test = number_of_images - n_train - n_valid\n",
    "    print(number_of_images, n_train, n_valid, n_test)\n",
    "    for idx, file in enumerate(files):\n",
    "        file_name = BASE_DIR + folder + file\n",
    "        if idx < n_train:\n",
    "            shutil.move(file_name, BASE_DIR + \"train/\" + names[folder_idx])\n",
    "        elif idx < n_train + n_valid:\n",
    "            shutil.move(file_name, BASE_DIR + \"val/\" + names[folder_idx])\n",
    "        else:\n",
    "            shutil.move(file_name, BASE_DIR + \"test/\" + names[folder_idx])\n",
    "\n",
    "# Generate batches of tensor image data with\n",
    "# optional real-time data augmentation.\n",
    "\n",
    "# preprocessing_function\n",
    "# rescale=1./255 -> [0,1]\n",
    "train_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "#    rotation_range=20,\n",
    "#    horizontal_flip=True,\n",
    "#    width_shift_range=0.2, height_shift_range=0.2,\n",
    "#    shear_range=0.2, zoom_range=0.2)\n",
    "\n",
    "valid_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_batches = train_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/train',\n",
    "    target_size=(256, 256),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names   \n",
    ")\n",
    "\n",
    "val_batches = valid_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/val',\n",
    "    target_size=(256, 256),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names\n",
    ")\n",
    "\n",
    "test_batches = test_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/test',\n",
    "    target_size=(256, 256),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names\n",
    ")\n",
    "\n",
    "train_batch = train_batches[0]\n",
    "print(train_batch[0].shape)\n",
    "print(train_batch[1])\n",
    "test_batch = test_batches[0]\n",
    "print(test_batch[0].shape)\n",
    "print(test_batch[1])\n",
    "\n",
    "\n",
    "def show(batch, pred_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(4):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(batch[0][i], cmap=plt.cm.binary)\n",
    "        # The CIFAR labels happen to be arrays, \n",
    "        # which is why you need the extra index\n",
    "        lbl = names[int(batch[1][i])]\n",
    "        if pred_labels is not None:\n",
    "            lbl += \"/ Pred:\" + names[int(pred_labels[i])]\n",
    "        plt.xlabel(lbl)\n",
    "    plt.show()\n",
    "\n",
    "show(test_batch)\n",
    "\n",
    "show(train_batch)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), strides=(1,1), padding=\"valid\", activation='relu', input_shape=(256, 256,3)))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(5))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
    "\n",
    "# training\n",
    "epochs = 30\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "history = model.fit(train_batches, validation_data=val_batches,\n",
    "                    callbacks=[early_stopping],\n",
    "                      epochs=epochs, verbose=2)\n",
    "\n",
    "\n",
    "model.save(\"lego_model.h5\")\n",
    "\n",
    "# plot loss and acc\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='valid loss')\n",
    "plt.grid()\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='valid acc')\n",
    "plt.grid()\n",
    "plt.legend(fontsize=15);\n",
    "\n",
    "# evaluate on test data\n",
    "model.evaluate(test_batches, verbose=2)\n",
    "\n",
    "\n",
    "# make some predictions\n",
    "predictions = model.predict(test_batches)\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(test_batches[0][1])\n",
    "print(labels[0:4])\n",
    "\n",
    "\n",
    "show(test_batches[0], labels[0:4])\n",
    "\n",
    "# Transfer Learning\n",
    "\n",
    "\n",
    "vgg_model = tf.keras.applications.vgg16.VGG16()\n",
    "print(type(vgg_model))\n",
    "vgg_model.summary()\n",
    "\n",
    "# try out different ones, e.g. MobileNetV2\n",
    "#tl_model = tf.keras.applications.MobileNetV2()\n",
    "#print(type(tl_model))\n",
    "#tl_model.summary()\n",
    "\n",
    "# convert to Sequential model, omit the last layer\n",
    "# this works with VGG16 because the structure is linear\n",
    "model = keras.models.Sequential()\n",
    "for layer in vgg_model.layers[0:-1]:\n",
    "    model.add(layer)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# set trainable=False for all layers\n",
    "# we don't want to train them again\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.summary()\n",
    "\n",
    "# add a last classification layer for our use case with 5 classes\n",
    "model.add(layers.Dense(5))\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
    "\n",
    "# get the preprocessing function of this model\n",
    "preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "# Generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_batches = train_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/train',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names   \n",
    ")\n",
    "\n",
    "val_batches = valid_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/val',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names\n",
    ")\n",
    "\n",
    "test_batches = test_gen.flow_from_directory(\n",
    "    'lego/star-wars-images/test',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='sparse',\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    color_mode=\"rgb\",\n",
    "    classes=names\n",
    ")\n",
    "epochs = 30\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "model.fit(train_batches, validation_data=val_batches,\n",
    "          callbacks=[early_stopping],\n",
    "          epochs=epochs, verbose=2)\n",
    "\n",
    "model.evaluate(test_batches, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Nets (RNN & LSTM & GRU)\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "# 28, 28 -> treat image as sequence\n",
    "# input_size=28\n",
    "# seq_length=28\n",
    "\n",
    "\n",
    "# RNN Outputs and states\n",
    "# ----------------------\n",
    "# By default, the output of a RNN layer contains a single vector per sample.\n",
    "# This vector is the RNN cell output corresponding to the last timestep, \n",
    "# containing information about the entire input sequence. \n",
    "# The shape of this output is (N, units)\n",
    "# \n",
    "# A RNN layer can also return the entire sequence of outputs \n",
    "# for each sample (one vector per timestep per sample), \n",
    "# if you set return_sequences=True. The shape of this output \n",
    "# is (N, timesteps, units).\n",
    "\n",
    "\n",
    "# model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input(shape=(28,28))) # seq_length, input_size\n",
    "#model.add(layers.SimpleRNN(128, return_sequences=True, activation='relu')) # N, 28, 128\n",
    "model.add(layers.LSTM(128, return_sequences=False, activation='relu')) # N, 128\n",
    "model.add(layers.Dense(10))\n",
    "print(model.summary())\n",
    "\n",
    "# loss and optimizer\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "# training\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2)\n",
    "\n",
    "# evaulate\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Tutorial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# https://www.kaggle.com/c/nlp-getting-started : NLP Disaster Tweets\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\MahendraVishnuPrabha\\\\Downloads\\\\TensorFlow\\\\test.csv\")\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.head()\n",
    "\n",
    "print((df.target == 1).sum()) # Disaster\n",
    "print((df.target == 0).sum()) # No Disaster\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "\n",
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break\n",
    "\n",
    "\n",
    "df[\"text\"] = df.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df[\"text\"] = df.text.map(remove_punct)\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as the, a, an, in) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "stop\n",
    "\n",
    "df[\"text\"] = df.text.map(remove_stopwords)\n",
    "df.text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(df.text)\n",
    "\n",
    "len(counter)\n",
    "\n",
    "counter\n",
    "\n",
    "counter.most_common(5)\n",
    "\n",
    "num_unique_words = len(counter)\n",
    "\n",
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "train_sentences = train_df.text.to_numpy()\n",
    "train_labels = train_df.target.to_numpy()\n",
    "val_sentences = val_df.text.to_numpy()\n",
    "val_labels = val_df.target.to_numpy()\n",
    "\n",
    "train_sentences.shape, val_sentences.shape\n",
    "\n",
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training\n",
    "\n",
    "# each word has unique index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "\n",
    "print(train_sentences[10:15])\n",
    "print(train_sequences[10:15])\n",
    "\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape\n",
    "\n",
    "\n",
    "train_padded[10]\n",
    "\n",
    "print(train_sentences[10])\n",
    "print(train_sequences[10])\n",
    "print(train_padded[10])\n",
    "\n",
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "reverse_word_index\n",
    "\n",
    "\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "\n",
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "print(train_sequences[10])\n",
    "print(decoded_text)\n",
    "\n",
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "model.fit(train_padded, train_labels, epochs=20, validation_data=(val_padded, val_labels), verbose=2)\n",
    "\n",
    "\n",
    "predictions = model.predict(train_padded)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "print(train_sentences[10:20])\n",
    "\n",
    "print(train_labels[10:20])\n",
    "print(predictions[10:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
